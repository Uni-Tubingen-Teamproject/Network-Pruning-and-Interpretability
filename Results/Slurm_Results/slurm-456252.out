JobId=456252 JobName=name
   UserId=wzz745(4834) GroupId=wichmann(4014) MCS_label=N/A
   Priority=78580 Nice=0 Account=wichmann QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:01 TimeLimit=3-00:00:00 TimeMin=N/A
   SubmitTime=2024-06-28T15:10:25 EligibleTime=2024-06-28T15:10:25
   AccrueTime=2024-06-28T15:10:25
   StartTime=2024-06-28T15:10:25 EndTime=2024-07-01T15:10:25 Deadline=N/A
   PreemptEligibleTime=2024-06-28T15:11:25 PreemptTime=None
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-06-28T15:10:25 Scheduler=Main
   Partition=2080-galvani AllocNode:Sid=galvani-slurmctl:3578282
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=galvani-cn120
   BatchHost=galvani-cn120
   NumNodes=1 NumCPUs=8 NumTasks=1 CPUs/Task=8 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=8,mem=40G,node=1,billing=2,gres/gpu=1
   AllocTRES=cpu=8,mem=40G,node=1,billing=2,gres/gpu=1,gres/gpu:rtx2080ti=1
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=8 MinMemoryNode=40G MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/ffcv.sh
   WorkDir=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability
   StdErr=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/slurm-456252.out
   StdIn=/dev/null
   StdOut=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/slurm-456252.out
   Power=
   TresPerNode=gres:gpu:1
   MailUser=jonathan.sakouhi@gmail.com MailType=BEGIN,END,FAIL
   

/usr/local/lib/python3.10/dist-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.
  warnings.warn(problem)
Train loader created in 10.258825778961182 seconds
Using cache found in /home/wichmann/wzz745/.cache/torch/hub/pytorch_vision_v0.10.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/googlenet.py:341: UserWarning: auxiliary heads in the pretrained googlenet model are NOT pretrained, so make sure to train them
  warnings.warn(
wandb: Currently logged in as: jonathan-vonrad (jonathan-von-rad). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/wichmann/wzz745/.netrc
wandb: wandb version 0.17.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/wichmann/wzz745/Network-Pruning-and-Interpretability/wandb/run-20240628_151057-jzmgsxq8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-surf-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jonathan-von-rad/my-awesome-project
wandb: üöÄ View run at https://wandb.ai/jonathan-von-rad/my-awesome-project/runs/jzmgsxq8
Train loader created in 0.1790323257446289 seconds
Training for 10 epochs with learning rate 0.01 and optimizer <class 'torch.optim.sgd.SGD'> and scheduler <class 'torch.optim.lr_scheduler.ExponentialLR'>

########## Specific Local Structured L1 Pruning ##########

Accuracy before: 0.69938

------------------- Pruning Modules with 0.2 -------------------

Module: inception3a.branch1.conv, Pruning Rate: 0.2
Module: inception3a.branch2.0.conv, Pruning Rate: 0.2
Module: inception3a.branch2.1.conv, Pruning Rate: 0.2
Module: inception3a.branch3.0.conv, Pruning Rate: 0.2
Module: inception3a.branch3.1.conv, Pruning Rate: 0.2
Module: inception3a.branch4.1.conv, Pruning Rate: 0.2
Module: inception3b.branch1.conv, Pruning Rate: 0.2
Module: inception3b.branch2.0.conv, Pruning Rate: 0.2
Module: inception3b.branch2.1.conv, Pruning Rate: 0.2
Module: inception3b.branch3.0.conv, Pruning Rate: 0.2
Module: inception3b.branch3.1.conv, Pruning Rate: 0.2
Module: inception3b.branch4.1.conv, Pruning Rate: 0.2
Module: inception4a.branch1.conv, Pruning Rate: 0.2
Module: inception4a.branch2.0.conv, Pruning Rate: 0.2
Module: inception4a.branch2.1.conv, Pruning Rate: 0.2
Module: inception4a.branch3.0.conv, Pruning Rate: 0.2
Module: inception4a.branch3.1.conv, Pruning Rate: 0.2
Module: inception4a.branch4.1.conv, Pruning Rate: 0.2
Module: inception4b.branch1.conv, Pruning Rate: 0.2
Module: inception4b.branch2.0.conv, Pruning Rate: 0.2
Module: inception4b.branch2.1.conv, Pruning Rate: 0.2
Module: inception4b.branch3.0.conv, Pruning Rate: 0.2
Module: inception4b.branch3.1.conv, Pruning Rate: 0.2
Module: inception4b.branch4.1.conv, Pruning Rate: 0.2
Module: inception4c.branch1.conv, Pruning Rate: 0.2
Module: inception4c.branch2.0.conv, Pruning Rate: 0.2
Module: inception4c.branch2.1.conv, Pruning Rate: 0.2
Module: inception4c.branch3.0.conv, Pruning Rate: 0.2
Module: inception4c.branch3.1.conv, Pruning Rate: 0.2
Module: inception4c.branch4.1.conv, Pruning Rate: 0.2
Module: inception4d.branch1.conv, Pruning Rate: 0.2
Module: inception4d.branch2.0.conv, Pruning Rate: 0.2
Module: inception4d.branch2.1.conv, Pruning Rate: 0.2
Module: inception4d.branch3.0.conv, Pruning Rate: 0.2
Module: inception4d.branch3.1.conv, Pruning Rate: 0.2
Module: inception4d.branch4.1.conv, Pruning Rate: 0.2
Module: inception4e.branch1.conv, Pruning Rate: 0.2
Module: inception4e.branch2.0.conv, Pruning Rate: 0.2
Module: inception4e.branch2.1.conv, Pruning Rate: 0.2
Module: inception4e.branch3.0.conv, Pruning Rate: 0.2
Module: inception4e.branch3.1.conv, Pruning Rate: 0.2
Module: inception4e.branch4.1.conv, Pruning Rate: 0.2
Module: inception5a.branch1.conv, Pruning Rate: 0.2
Module: inception5a.branch2.0.conv, Pruning Rate: 0.2
Module: inception5a.branch2.1.conv, Pruning Rate: 0.2
Module: inception5a.branch3.0.conv, Pruning Rate: 0.2
Module: inception5a.branch3.1.conv, Pruning Rate: 0.2
Module: inception5a.branch4.1.conv, Pruning Rate: 0.2
Module: inception5b.branch1.conv, Pruning Rate: 0.2
Module: inception5b.branch2.0.conv, Pruning Rate: 0.2
Module: inception5b.branch2.1.conv, Pruning Rate: 0.2
Module: inception5b.branch3.0.conv, Pruning Rate: 0.2
Module: inception5b.branch3.1.conv, Pruning Rate: 0.2
Module: inception5b.branch4.1.conv, Pruning Rate: 0.2

--------------------------------------------------------

Actual Pruning Rate: 0.19134368949424185
Accuracy after pruning every module with 0.2:  0.00368
Epoch [1/10], Training Loss: 5.24332099082837, Training Loss w/o Aux: 2.100388407004512, Learning Rate: 0.009000000000000001, Validation Accuracy: 0.4316
Epoch [2/10], Training Loss: 4.35384885840083, Training Loss w/o Aux: 1.9260879513242883, Learning Rate: 0.008100000000000001, Validation Accuracy: 0.46962
Epoch [3/10], Training Loss: 4.088918730671654, Training Loss w/o Aux: 1.8673251898737264, Learning Rate: 0.007290000000000001, Validation Accuracy: 0.51112
Epoch [4/10], Training Loss: 3.9335649512080666, Training Loss w/o Aux: 1.8268407674111595, Learning Rate: 0.006561000000000002, Validation Accuracy: 0.52176
Epoch [5/10], Training Loss: 3.833777053740394, Training Loss w/o Aux: 1.8015878226257058, Learning Rate: 0.005904900000000002, Validation Accuracy: 0.53956
Epoch [6/10], Training Loss: 3.7526674865902168, Training Loss w/o Aux: 1.7765002783534172, Learning Rate: 0.005314410000000002, Validation Accuracy: 0.5538
Epoch [7/10], Training Loss: 3.684837769432994, Training Loss w/o Aux: 1.7531984324268985, Learning Rate: 0.004782969000000002, Validation Accuracy: 0.56162
Epoch [8/10], Training Loss: 3.631220188063691, Training Loss w/o Aux: 1.735329703013134, Learning Rate: 0.004304672100000002, Validation Accuracy: 0.571
Epoch [9/10], Training Loss: 3.581798462115488, Training Loss w/o Aux: 1.7157994107011911, Learning Rate: 0.003874204890000002, Validation Accuracy: 0.58528
Epoch [10/10], Training Loss: 3.5428773784675562, Training Loss w/o Aux: 1.701409849256816, Learning Rate: 0.003486784401000002, Validation Accuracy: 0.58432
Accuracy after retraining: 0.58432
removing pruning masks ...
Final pruned and retrained model saved as pruned_0.2_local_structured_retrained_10_epochs_model.pth

Resetting the model to the initial state ...

------------------- Pruning Modules with 0.4 -------------------

Module: inception3a.branch1.conv, Pruning Rate: 0.4
Module: inception3a.branch2.0.conv, Pruning Rate: 0.4
Module: inception3a.branch2.1.conv, Pruning Rate: 0.4
Module: inception3a.branch3.0.conv, Pruning Rate: 0.4
Module: inception3a.branch3.1.conv, Pruning Rate: 0.4
Module: inception3a.branch4.1.conv, Pruning Rate: 0.4
Module: inception3b.branch1.conv, Pruning Rate: 0.4
Module: inception3b.branch2.0.conv, Pruning Rate: 0.4
Module: inception3b.branch2.1.conv, Pruning Rate: 0.4
Module: inception3b.branch3.0.conv, Pruning Rate: 0.4
Module: inception3b.branch3.1.conv, Pruning Rate: 0.4
Module: inception3b.branch4.1.conv, Pruning Rate: 0.4
Module: inception4a.branch1.conv, Pruning Rate: 0.4
Module: inception4a.branch2.0.conv, Pruning Rate: 0.4
Module: inception4a.branch2.1.conv, Pruning Rate: 0.4
Module: inception4a.branch3.0.conv, Pruning Rate: 0.4
Module: inception4a.branch3.1.conv, Pruning Rate: 0.4
Module: inception4a.branch4.1.conv, Pruning Rate: 0.4
Module: inception4b.branch1.conv, Pruning Rate: 0.4
Module: inception4b.branch2.0.conv, Pruning Rate: 0.4
Module: inception4b.branch2.1.conv, Pruning Rate: 0.4
Module: inception4b.branch3.0.conv, Pruning Rate: 0.4
Module: inception4b.branch3.1.conv, Pruning Rate: 0.4
Module: inception4b.branch4.1.conv, Pruning Rate: 0.4
Module: inception4c.branch1.conv, Pruning Rate: 0.4
Module: inception4c.branch2.0.conv, Pruning Rate: 0.4
Module: inception4c.branch2.1.conv, Pruning Rate: 0.4
Module: inception4c.branch3.0.conv, Pruning Rate: 0.4
Module: inception4c.branch3.1.conv, Pruning Rate: 0.4
Module: inception4c.branch4.1.conv, Pruning Rate: 0.4
Module: inception4d.branch1.conv, Pruning Rate: 0.4
Module: inception4d.branch2.0.conv, Pruning Rate: 0.4
Module: inception4d.branch2.1.conv, Pruning Rate: 0.4
Module: inception4d.branch3.0.conv, Pruning Rate: 0.4
Module: inception4d.branch3.1.conv, Pruning Rate: 0.4
Module: inception4d.branch4.1.conv, Pruning Rate: 0.4
Module: inception4e.branch1.conv, Pruning Rate: 0.4
Module: inception4e.branch2.0.conv, Pruning Rate: 0.4
Module: inception4e.branch2.1.conv, Pruning Rate: 0.4
Module: inception4e.branch3.0.conv, Pruning Rate: 0.4
Module: inception4e.branch3.1.conv, Pruning Rate: 0.4
Module: inception4e.branch4.1.conv, Pruning Rate: 0.4
Module: inception5a.branch1.conv, Pruning Rate: 0.4
Module: inception5a.branch2.0.conv, Pruning Rate: 0.4
Module: inception5a.branch2.1.conv, Pruning Rate: 0.4
Module: inception5a.branch3.0.conv, Pruning Rate: 0.4
Module: inception5a.branch3.1.conv, Pruning Rate: 0.4
Module: inception5a.branch4.1.conv, Pruning Rate: 0.4
Module: inception5b.branch1.conv, Pruning Rate: 0.4
Module: inception5b.branch2.0.conv, Pruning Rate: 0.4
Module: inception5b.branch2.1.conv, Pruning Rate: 0.4
Module: inception5b.branch3.0.conv, Pruning Rate: 0.4
Module: inception5b.branch3.1.conv, Pruning Rate: 0.4
Module: inception5b.branch4.1.conv, Pruning Rate: 0.4

--------------------------------------------------------

Actual Pruning Rate: 0.3822005349688308
Accuracy after pruning every module with 0.4:  0.001
Epoch [1/10], Training Loss: 5.9173343553208655, Training Loss w/o Aux: 2.721426283185305, Learning Rate: 0.009000000000000001, Validation Accuracy: 0.33196
Epoch [2/10], Training Loss: 4.811666657843901, Training Loss w/o Aux: 2.3033850461122505, Learning Rate: 0.008100000000000001, Validation Accuracy: 0.38926
Epoch [3/10], Training Loss: 4.506393069879839, Training Loss w/o Aux: 2.2008591128438963, Learning Rate: 0.007290000000000001, Validation Accuracy: 0.4252
Epoch [4/10], Training Loss: 4.329456243973319, Training Loss w/o Aux: 2.1376935462754427, Learning Rate: 0.006561000000000002, Validation Accuracy: 0.44746
Epoch [5/10], Training Loss: 4.212642640450079, Training Loss w/o Aux: 2.0950872108216694, Learning Rate: 0.005904900000000002, Validation Accuracy: 0.47784
Epoch [6/10], Training Loss: 4.117103994613237, Training Loss w/o Aux: 2.0574035257449435, Learning Rate: 0.005314410000000002, Validation Accuracy: 0.49232
Epoch [7/10], Training Loss: 4.046714451539551, Training Loss w/o Aux: 2.029009815457699, Learning Rate: 0.004782969000000002, Validation Accuracy: 0.49126
Epoch [8/10], Training Loss: 3.9893421689188915, Training Loss w/o Aux: 2.005103144651408, Learning Rate: 0.004304672100000002, Validation Accuracy: 0.50852
Epoch [9/10], Training Loss: 3.941432656364277, Training Loss w/o Aux: 1.9854775919686998, Learning Rate: 0.003874204890000002, Validation Accuracy: 0.52248
Epoch [10/10], Training Loss: 3.897338296664632, Training Loss w/o Aux: 1.9668310187605904, Learning Rate: 0.003486784401000002, Validation Accuracy: 0.53764
Accuracy after retraining: 0.53764
removing pruning masks ...
Final pruned and retrained model saved as pruned_0.4_local_structured_retrained_10_epochs_model.pth

Resetting the model to the initial state ...

------------------- Pruning Modules with 0.6 -------------------

Module: inception3a.branch1.conv, Pruning Rate: 0.6
Module: inception3a.branch2.0.conv, Pruning Rate: 0.6
Module: inception3a.branch2.1.conv, Pruning Rate: 0.6
Module: inception3a.branch3.0.conv, Pruning Rate: 0.6
Module: inception3a.branch3.1.conv, Pruning Rate: 0.6
Module: inception3a.branch4.1.conv, Pruning Rate: 0.6
Module: inception3b.branch1.conv, Pruning Rate: 0.6
Module: inception3b.branch2.0.conv, Pruning Rate: 0.6
Module: inception3b.branch2.1.conv, Pruning Rate: 0.6
Module: inception3b.branch3.0.conv, Pruning Rate: 0.6
Module: inception3b.branch3.1.conv, Pruning Rate: 0.6
Module: inception3b.branch4.1.conv, Pruning Rate: 0.6
Module: inception4a.branch1.conv, Pruning Rate: 0.6
Module: inception4a.branch2.0.conv, Pruning Rate: 0.6
Module: inception4a.branch2.1.conv, Pruning Rate: 0.6
Module: inception4a.branch3.0.conv, Pruning Rate: 0.6
Module: inception4a.branch3.1.conv, Pruning Rate: 0.6
Module: inception4a.branch4.1.conv, Pruning Rate: 0.6
Module: inception4b.branch1.conv, Pruning Rate: 0.6
Module: inception4b.branch2.0.conv, Pruning Rate: 0.6
Module: inception4b.branch2.1.conv, Pruning Rate: 0.6
Module: inception4b.branch3.0.conv, Pruning Rate: 0.6
Module: inception4b.branch3.1.conv, Pruning Rate: 0.6
Module: inception4b.branch4.1.conv, Pruning Rate: 0.6
Module: inception4c.branch1.conv, Pruning Rate: 0.6
Module: inception4c.branch2.0.conv, Pruning Rate: 0.6
Module: inception4c.branch2.1.conv, Pruning Rate: 0.6
Module: inception4c.branch3.0.conv, Pruning Rate: 0.6
Module: inception4c.branch3.1.conv, Pruning Rate: 0.6
Module: inception4c.branch4.1.conv, Pruning Rate: 0.6
Module: inception4d.branch1.conv, Pruning Rate: 0.6
Module: inception4d.branch2.0.conv, Pruning Rate: 0.6
Module: inception4d.branch2.1.conv, Pruning Rate: 0.6
Module: inception4d.branch3.0.conv, Pruning Rate: 0.6
Module: inception4d.branch3.1.conv, Pruning Rate: 0.6
Module: inception4d.branch4.1.conv, Pruning Rate: 0.6
Module: inception4e.branch1.conv, Pruning Rate: 0.6
Module: inception4e.branch2.0.conv, Pruning Rate: 0.6
Module: inception4e.branch2.1.conv, Pruning Rate: 0.6
Module: inception4e.branch3.0.conv, Pruning Rate: 0.6
Module: inception4e.branch3.1.conv, Pruning Rate: 0.6
Module: inception4e.branch4.1.conv, Pruning Rate: 0.6
Module: inception5a.branch1.conv, Pruning Rate: 0.6
Module: inception5a.branch2.0.conv, Pruning Rate: 0.6
Module: inception5a.branch2.1.conv, Pruning Rate: 0.6
Module: inception5a.branch3.0.conv, Pruning Rate: 0.6
Module: inception5a.branch3.1.conv, Pruning Rate: 0.6
Module: inception5a.branch4.1.conv, Pruning Rate: 0.6
Module: inception5b.branch1.conv, Pruning Rate: 0.6
Module: inception5b.branch2.0.conv, Pruning Rate: 0.6
Module: inception5b.branch2.1.conv, Pruning Rate: 0.6
Module: inception5b.branch3.0.conv, Pruning Rate: 0.6
Module: inception5b.branch3.1.conv, Pruning Rate: 0.6
Module: inception5b.branch4.1.conv, Pruning Rate: 0.6

--------------------------------------------------------

Actual Pruning Rate: 0.5728195543418653
Accuracy after pruning every module with 0.6:  0.001
Epoch [1/10], Training Loss: 7.011310318369336, Training Loss w/o Aux: 3.7839631348273675, Learning Rate: 0.009000000000000001, Validation Accuracy: 0.1972
Epoch [2/10], Training Loss: 5.6041635495106865, Training Loss w/o Aux: 3.007331878682309, Learning Rate: 0.008100000000000001, Validation Accuracy: 0.27638
Epoch [3/10], Training Loss: 5.217315661584429, Training Loss w/o Aux: 2.811249022813024, Learning Rate: 0.007290000000000001, Validation Accuracy: 0.31104
Epoch [4/10], Training Loss: 4.993759535852281, Training Loss w/o Aux: 2.697240798167742, Learning Rate: 0.006561000000000002, Validation Accuracy: 0.34396
Epoch [5/10], Training Loss: 4.8480468989966905, Training Loss w/o Aux: 2.624111610569431, Learning Rate: 0.005904900000000002, Validation Accuracy: 0.35872
Epoch [6/10], Training Loss: 4.7370466109529366, Training Loss w/o Aux: 2.566894553629284, Learning Rate: 0.005314410000000002, Validation Accuracy: 0.38468
Epoch [7/10], Training Loss: 4.658861832180894, Training Loss w/o Aux: 2.5288238483943095, Learning Rate: 0.004782969000000002, Validation Accuracy: 0.40782
Epoch [8/10], Training Loss: 4.58870726920875, Training Loss w/o Aux: 2.4923305367003765, Learning Rate: 0.004304672100000002, Validation Accuracy: 0.41846
Epoch [9/10], Training Loss: 4.530745809472921, Training Loss w/o Aux: 2.462454429705644, Learning Rate: 0.003874204890000002, Validation Accuracy: 0.43184
Epoch [10/10], Training Loss: 4.484399632600557, Training Loss w/o Aux: 2.438756773647865, Learning Rate: 0.003486784401000002, Validation Accuracy: 0.44184
Accuracy after retraining: 0.44184
removing pruning masks ...
Final pruned and retrained model saved as pruned_0.6_local_structured_retrained_10_epochs_model.pth

Resetting the model to the initial state ...

------------------- Pruning Modules with 0.8 -------------------

Module: inception3a.branch1.conv, Pruning Rate: 0.8
Module: inception3a.branch2.0.conv, Pruning Rate: 0.8
Module: inception3a.branch2.1.conv, Pruning Rate: 0.8
Module: inception3a.branch3.0.conv, Pruning Rate: 0.8
Module: inception3a.branch3.1.conv, Pruning Rate: 0.8
Module: inception3a.branch4.1.conv, Pruning Rate: 0.8
Module: inception3b.branch1.conv, Pruning Rate: 0.8
Module: inception3b.branch2.0.conv, Pruning Rate: 0.8
Module: inception3b.branch2.1.conv, Pruning Rate: 0.8
Module: inception3b.branch3.0.conv, Pruning Rate: 0.8
Module: inception3b.branch3.1.conv, Pruning Rate: 0.8
Module: inception3b.branch4.1.conv, Pruning Rate: 0.8
Module: inception4a.branch1.conv, Pruning Rate: 0.8
Module: inception4a.branch2.0.conv, Pruning Rate: 0.8
Module: inception4a.branch2.1.conv, Pruning Rate: 0.8
Module: inception4a.branch3.0.conv, Pruning Rate: 0.8
Module: inception4a.branch3.1.conv, Pruning Rate: 0.8
Module: inception4a.branch4.1.conv, Pruning Rate: 0.8
Module: inception4b.branch1.conv, Pruning Rate: 0.8
Module: inception4b.branch2.0.conv, Pruning Rate: 0.8
Module: inception4b.branch2.1.conv, Pruning Rate: 0.8
Module: inception4b.branch3.0.conv, Pruning Rate: 0.8
Module: inception4b.branch3.1.conv, Pruning Rate: 0.8
Module: inception4b.branch4.1.conv, Pruning Rate: 0.8
Module: inception4c.branch1.conv, Pruning Rate: 0.8
Module: inception4c.branch2.0.conv, Pruning Rate: 0.8
Module: inception4c.branch2.1.conv, Pruning Rate: 0.8
Module: inception4c.branch3.0.conv, Pruning Rate: 0.8
Module: inception4c.branch3.1.conv, Pruning Rate: 0.8
Module: inception4c.branch4.1.conv, Pruning Rate: 0.8
Module: inception4d.branch1.conv, Pruning Rate: 0.8
Module: inception4d.branch2.0.conv, Pruning Rate: 0.8
Module: inception4d.branch2.1.conv, Pruning Rate: 0.8
Module: inception4d.branch3.0.conv, Pruning Rate: 0.8
Module: inception4d.branch3.1.conv, Pruning Rate: 0.8
Module: inception4d.branch4.1.conv, Pruning Rate: 0.8
Module: inception4e.branch1.conv, Pruning Rate: 0.8
Module: inception4e.branch2.0.conv, Pruning Rate: 0.8
Module: inception4e.branch2.1.conv, Pruning Rate: 0.8
Module: inception4e.branch3.0.conv, Pruning Rate: 0.8
Module: inception4e.branch3.1.conv, Pruning Rate: 0.8
Module: inception4e.branch4.1.conv, Pruning Rate: 0.8
Module: inception5a.branch1.conv, Pruning Rate: 0.8
Module: inception5a.branch2.0.conv, Pruning Rate: 0.8
Module: inception5a.branch2.1.conv, Pruning Rate: 0.8
Module: inception5a.branch3.0.conv, Pruning Rate: 0.8
Module: inception5a.branch3.1.conv, Pruning Rate: 0.8
Module: inception5a.branch4.1.conv, Pruning Rate: 0.8
Module: inception5b.branch1.conv, Pruning Rate: 0.8
Module: inception5b.branch2.0.conv, Pruning Rate: 0.8
Module: inception5b.branch2.1.conv, Pruning Rate: 0.8
Module: inception5b.branch3.0.conv, Pruning Rate: 0.8
Module: inception5b.branch3.1.conv, Pruning Rate: 0.8
Module: inception5b.branch4.1.conv, Pruning Rate: 0.8

--------------------------------------------------------

Actual Pruning Rate: 0.7636763998164542
Accuracy after pruning every module with 0.8:  0.001
Epoch [1/10], Training Loss: 8.191856154315014, Training Loss w/o Aux: 4.837532058987945, Learning Rate: 0.009000000000000001, Validation Accuracy: 0.08174
Epoch [2/10], Training Loss: 6.855147227360564, Training Loss w/o Aux: 4.042883868721985, Learning Rate: 0.008100000000000001, Validation Accuracy: 0.13714
Epoch [3/10], Training Loss: 6.422472130753251, Training Loss w/o Aux: 3.7906363011121105, Learning Rate: 0.007290000000000001, Validation Accuracy: 0.16664
Epoch [4/10], Training Loss: 6.1643225974476366, Training Loss w/o Aux: 3.6376362440052277, Learning Rate: 0.006561000000000002, Validation Accuracy: 0.20028
Epoch [5/10], Training Loss: 5.992396311607974, Training Loss w/o Aux: 3.5369609602563137, Learning Rate: 0.005904900000000002, Validation Accuracy: 0.20148
Epoch [6/10], Training Loss: 5.870381627743603, Training Loss w/o Aux: 3.467696596971341, Learning Rate: 0.005314410000000002, Validation Accuracy: 0.2354
Epoch [7/10], Training Loss: 5.775480703012202, Training Loss w/o Aux: 3.4134356918386413, Learning Rate: 0.004782969000000002, Validation Accuracy: 0.25618
Epoch [8/10], Training Loss: 5.693546475200747, Training Loss w/o Aux: 3.366930305761276, Learning Rate: 0.004304672100000002, Validation Accuracy: 0.26332
Epoch [9/10], Training Loss: 5.627742920594564, Training Loss w/o Aux: 3.3290725613774232, Learning Rate: 0.003874204890000002, Validation Accuracy: 0.26906
Epoch [10/10], Training Loss: 5.5722907971807, Training Loss w/o Aux: 3.2976616897453424, Learning Rate: 0.003486784401000002, Validation Accuracy: 0.29486
Accuracy after retraining: 0.29486
removing pruning masks ...
Final pruned and retrained model saved as pruned_0.8_local_structured_retrained_10_epochs_model.pth

Resetting the model to the initial state ...

------------------- Pruning Modules with 0.2 -------------------

Module: inception3a.branch1.conv, Pruning Rate: 0.2
Module: inception3a.branch2.0.conv, Pruning Rate: 0.2
Module: inception3a.branch2.1.conv, Pruning Rate: 0.2
Module: inception3a.branch3.0.conv, Pruning Rate: 0.2
Module: inception3a.branch3.1.conv, Pruning Rate: 0.2
Module: inception3a.branch4.1.conv, Pruning Rate: 0.2
Module: inception3b.branch1.conv, Pruning Rate: 0.2
Module: inception3b.branch2.0.conv, Pruning Rate: 0.2
Module: inception3b.branch2.1.conv, Pruning Rate: 0.2
Module: inception3b.branch3.0.conv, Pruning Rate: 0.2
Module: inception3b.branch3.1.conv, Pruning Rate: 0.2
Module: inception3b.branch4.1.conv, Pruning Rate: 0.2
Module: inception4a.branch1.conv, Pruning Rate: 0.2
Module: inception4a.branch2.0.conv, Pruning Rate: 0.2
Module: inception4a.branch2.1.conv, Pruning Rate: 0.2
Module: inception4a.branch3.0.conv, Pruning Rate: 0.2
Module: inception4a.branch3.1.conv, Pruning Rate: 0.2
Module: inception4a.branch4.1.conv, Pruning Rate: 0.2
Module: inception4b.branch1.conv, Pruning Rate: 0.2
Module: inception4b.branch2.0.conv, Pruning Rate: 0.2
Module: inception4b.branch2.1.conv, Pruning Rate: 0.2
Module: inception4b.branch3.0.conv, Pruning Rate: 0.2
Module: inception4b.branch3.1.conv, Pruning Rate: 0.2
Module: inception4b.branch4.1.conv, Pruning Rate: 0.2
Module: inception4c.branch1.conv, Pruning Rate: 0.2
Module: inception4c.branch2.0.conv, Pruning Rate: 0.2
Module: inception4c.branch2.1.conv, Pruning Rate: 0.2
Module: inception4c.branch3.0.conv, Pruning Rate: 0.2
Module: inception4c.branch3.1.conv, Pruning Rate: 0.2
Module: inception4c.branch4.1.conv, Pruning Rate: 0.2
Module: inception4d.branch1.conv, Pruning Rate: 0.2
Module: inception4d.branch2.0.conv, Pruning Rate: 0.2
Module: inception4d.branch2.1.conv, Pruning Rate: 0.2
Module: inception4d.branch3.0.conv, Pruning Rate: 0.2
Module: inception4d.branch3.1.conv, Pruning Rate: 0.2
Module: inception4d.branch4.1.conv, Pruning Rate: 0.2
Module: inception4e.branch1.conv, Pruning Rate: 0.2
Module: inception4e.branch2.0.conv, Pruning Rate: 0.2
Module: inception4e.branch2.1.conv, Pruning Rate: 0.2
Module: inception4e.branch3.0.conv, Pruning Rate: 0.2
Module: inception4e.branch3.1.conv, Pruning Rate: 0.2
Module: inception4e.branch4.1.conv, Pruning Rate: 0.2
Module: inception5a.branch1.conv, Pruning Rate: 0.2
Module: inception5a.branch2.0.conv, Pruning Rate: 0.2
Module: inception5a.branch2.1.conv, Pruning Rate: 0.2
Module: inception5a.branch3.0.conv, Pruning Rate: 0.2
Module: inception5a.branch3.1.conv, Pruning Rate: 0.2
Module: inception5a.branch4.1.conv, Pruning Rate: 0.2
Module: inception5b.branch1.conv, Pruning Rate: 0.2
Module: inception5b.branch2.0.conv, Pruning Rate: 0.2
Module: inception5b.branch2.1.conv, Pruning Rate: 0.2
Module: inception5b.branch3.0.conv, Pruning Rate: 0.2
Module: inception5b.branch3.1.conv, Pruning Rate: 0.2
Module: inception5b.branch4.1.conv, Pruning Rate: 0.2

--------------------------------------------------------

Actual Pruning Rate: 0.19134368949424185
Accuracy after pruning every module with 0.2:  0.00368
Epoch [1/50], Training Loss: 5.249660689700482, Training Loss w/o Aux: 2.103341454356233, Learning Rate: 0.009000000000000001, Validation Accuracy: 0.42424
Epoch [2/50], Training Loss: 4.357559040312358, Training Loss w/o Aux: 1.9267054581239587, Learning Rate: 0.008100000000000001, Validation Accuracy: 0.46828
Epoch [3/50], Training Loss: 4.088968753719415, Training Loss w/o Aux: 1.8681023314778342, Learning Rate: 0.007290000000000001, Validation Accuracy: 0.49668
Epoch [4/50], Training Loss: 3.939797688702482, Training Loss w/o Aux: 1.8325916100887047, Learning Rate: 0.006561000000000002, Validation Accuracy: 0.51854
Epoch [5/50], Training Loss: 3.8277718077554894, Training Loss w/o Aux: 1.797433540816073, Learning Rate: 0.005904900000000002, Validation Accuracy: 0.52046
Epoch [6/50], Training Loss: 3.748956304721297, Training Loss w/o Aux: 1.7741204732090148, Learning Rate: 0.005314410000000002, Validation Accuracy: 0.5483
Epoch [7/50], Training Loss: 3.68005711251723, Training Loss w/o Aux: 1.751026204945098, Learning Rate: 0.004782969000000002, Validation Accuracy: 0.55432
Epoch [8/50], Training Loss: 3.6345667518427445, Training Loss w/o Aux: 1.7382316136648395, Learning Rate: 0.004304672100000002, Validation Accuracy: 0.56402
Epoch [9/50], Training Loss: 3.586979907642105, Training Loss w/o Aux: 1.7191127132900184, Learning Rate: 0.003874204890000002, Validation Accuracy: 0.57942
Epoch [10/50], Training Loss: 3.544261839497231, Training Loss w/o Aux: 1.7016533674661638, Learning Rate: 0.003486784401000002, Validation Accuracy: 0.5858
Epoch [11/50], Training Loss: 3.5040996923636265, Training Loss w/o Aux: 1.6854811485490904, Learning Rate: 0.003138105960900002, Validation Accuracy: 0.60012
Epoch [12/50], Training Loss: 3.4711625668442516, Training Loss w/o Aux: 1.6718020738689725, Learning Rate: 0.0028242953648100018, Validation Accuracy: 0.60284
Epoch [13/50], Training Loss: 3.4421940459418052, Training Loss w/o Aux: 1.6585334350338872, Learning Rate: 0.0025418658283290017, Validation Accuracy: 0.6129
Epoch [14/50], Training Loss: 3.411152468038279, Training Loss w/o Aux: 1.644167845905905, Learning Rate: 0.0022876792454961017, Validation Accuracy: 0.61186
Epoch [15/50], Training Loss: 3.388419356304683, Training Loss w/o Aux: 1.6348318645129707, Learning Rate: 0.0020589113209464917, Validation Accuracy: 0.63634
Epoch [16/50], Training Loss: 3.367058258740763, Training Loss w/o Aux: 1.6239217366654386, Learning Rate: 0.0018530201888518425, Validation Accuracy: 0.63116
Epoch [17/50], Training Loss: 3.3447397866441078, Training Loss w/o Aux: 1.6138372626060706, Learning Rate: 0.0016677181699666583, Validation Accuracy: 0.64144
Epoch [18/50], Training Loss: 3.325166499916044, Training Loss w/o Aux: 1.6034707356099542, Learning Rate: 0.0015009463529699924, Validation Accuracy: 0.64364
Epoch [19/50], Training Loss: 3.3105117278518774, Training Loss w/o Aux: 1.597328643827889, Learning Rate: 0.0013508517176729932, Validation Accuracy: 0.64934
Epoch [20/50], Training Loss: 3.290510492756455, Training Loss w/o Aux: 1.5861733456998572, Learning Rate: 0.001215766545905694, Validation Accuracy: 0.6572
Epoch [21/50], Training Loss: 3.273828705227307, Training Loss w/o Aux: 1.5776891379235376, Learning Rate: 0.0010941898913151245, Validation Accuracy: 0.65954
Epoch [22/50], Training Loss: 3.260925939312014, Training Loss w/o Aux: 1.571146140066176, Learning Rate: 0.0009847709021836122, Validation Accuracy: 0.65912
Epoch [23/50], Training Loss: 3.2513454931577206, Training Loss w/o Aux: 1.5666372231496704, Learning Rate: 0.0008862938119652509, Validation Accuracy: 0.66788
Epoch [24/50], Training Loss: 3.2363603701064108, Training Loss w/o Aux: 1.5588686023931402, Learning Rate: 0.0007976644307687258, Validation Accuracy: 0.6728
Epoch [25/50], Training Loss: 3.220351654733617, Training Loss w/o Aux: 1.5490310099565825, Learning Rate: 0.0007178979876918532, Validation Accuracy: 0.6758
Epoch [26/50], Training Loss: 3.210396468192741, Training Loss w/o Aux: 1.543126978095517, Learning Rate: 0.0006461081889226679, Validation Accuracy: 0.6764
Epoch [27/50], Training Loss: 3.2038790124650602, Training Loss w/o Aux: 1.5399558424818633, Learning Rate: 0.0005814973700304011, Validation Accuracy: 0.67854
Epoch [28/50], Training Loss: 3.1961615224951263, Training Loss w/o Aux: 1.5365627159759325, Learning Rate: 0.0005233476330273611, Validation Accuracy: 0.68114
Epoch [29/50], Training Loss: 3.1852396227927793, Training Loss w/o Aux: 1.5294795335857694, Learning Rate: 0.000471012869724625, Validation Accuracy: 0.68422
Epoch [30/50], Training Loss: 3.18152023986308, Training Loss w/o Aux: 1.5280575919596748, Learning Rate: 0.0004239115827521625, Validation Accuracy: 0.68638
Epoch [31/50], Training Loss: 3.169633858877291, Training Loss w/o Aux: 1.521302446944812, Learning Rate: 0.00038152042447694626, Validation Accuracy: 0.68588
Epoch [32/50], Training Loss: 3.1643395408167683, Training Loss w/o Aux: 1.5184184876054254, Learning Rate: 0.00034336838202925164, Validation Accuracy: 0.68654
Epoch [33/50], Training Loss: 3.156836577950092, Training Loss w/o Aux: 1.5137663204206169, Learning Rate: 0.0003090315438263265, Validation Accuracy: 0.68928
slurmstepd: error: *** STEP 456252.0 ON galvani-cn120 CANCELLED AT 2024-06-29T16:20:55 ***
slurmstepd: error: *** JOB 456252 ON galvani-cn120 CANCELLED AT 2024-06-29T16:20:55 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
