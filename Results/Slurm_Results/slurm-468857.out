JobId=468857 JobName=name
   UserId=wzz745(4834) GroupId=wichmann(4014) MCS_label=N/A
   Priority=66714 Nice=0 Account=wichmann QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:01 TimeLimit=3-00:00:00 TimeMin=N/A
   SubmitTime=2024-07-07T14:20:38 EligibleTime=2024-07-07T14:20:38
   AccrueTime=2024-07-07T14:20:38
   StartTime=2024-07-07T14:20:38 EndTime=2024-07-10T14:20:38 Deadline=N/A
   PreemptEligibleTime=2024-07-07T14:21:38 PreemptTime=None
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-07-07T14:20:38 Scheduler=Main
   Partition=2080-galvani AllocNode:Sid=galvani-slurmctl:49140
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=galvani-cn110
   BatchHost=galvani-cn110
   NumNodes=1 NumCPUs=8 NumTasks=1 CPUs/Task=8 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=8,mem=40G,node=1,billing=2,gres/gpu=1
   AllocTRES=cpu=8,mem=40G,node=1,billing=2,gres/gpu=1,gres/gpu:rtx2080ti=1
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=8 MinMemoryNode=40G MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/ffcv.sh
   WorkDir=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability
   StdErr=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/slurm-468857.out
   StdIn=/dev/null
   StdOut=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/slurm-468857.out
   Power=
   TresPerNode=gres:gpu:1
   MailUser=jonathan.sakouhi@gmail.com MailType=BEGIN,END,FAIL
   

/usr/local/lib/python3.10/dist-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.
  warnings.warn(problem)
Train loader created in 11.801576614379883 seconds
Using cache found in /home/wichmann/wzz745/.cache/torch/hub/pytorch_vision_v0.10.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/googlenet.py:341: UserWarning: auxiliary heads in the pretrained googlenet model are NOT pretrained, so make sure to train them
  warnings.warn(
wandb: Currently logged in as: jonathan-vonrad (jonathan-von-rad). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/wichmann/wzz745/.netrc
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/wichmann/wzz745/Network-Pruning-and-Interpretability/wandb/run-20240707_142121-1ajuykyd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-donkey-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jonathan-von-rad/iterative-pruning-retraining
wandb: üöÄ View run at https://wandb.ai/jonathan-von-rad/iterative-pruning-retraining/runs/1ajuykyd
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Train loader created in 0.24640202522277832 seconds
Training for 60 epochs with learning rate 0.01 and optimizer SGD and scheduler ExponentialLR

########## Specific Local Unstructured L1 Pruning ##########

Accuracy before: 0.69938
Non-zero params before Pruning: 5718464, Total params: 5718464

------------------- Pruning Modules -------------------

Module: inception3a.branch1.conv, Pruning Rate: 0.13
Module: inception3a.branch2.0.conv, Pruning Rate: 0.16
Module: inception3a.branch2.1.conv, Pruning Rate: 0.21
Module: inception3a.branch3.0.conv, Pruning Rate: 0.21
Module: inception3a.branch3.1.conv, Pruning Rate: 0.23
Module: inception3a.branch4.1.conv, Pruning Rate: 0.16
Module: inception3b.branch1.conv, Pruning Rate: 0.18
Module: inception3b.branch2.0.conv, Pruning Rate: 0.18
Module: inception3b.branch2.1.conv, Pruning Rate: 0.21
Module: inception3b.branch3.0.conv, Pruning Rate: 0.23
Module: inception3b.branch3.1.conv, Pruning Rate: 0.21
Module: inception3b.branch4.1.conv, Pruning Rate: 0.18
Module: inception4a.branch1.conv, Pruning Rate: 0.16
Module: inception4a.branch2.0.conv, Pruning Rate: 0.21
Module: inception4a.branch2.1.conv, Pruning Rate: 0.23
Module: inception4a.branch3.0.conv, Pruning Rate: 0.23
Module: inception4a.branch3.1.conv, Pruning Rate: 0.16
Module: inception4a.branch4.1.conv, Pruning Rate: 0.16
Module: inception4b.branch1.conv, Pruning Rate: 0.18
Module: inception4b.branch2.0.conv, Pruning Rate: 0.23
Module: inception4b.branch2.1.conv, Pruning Rate: 0.21
Module: inception4b.branch3.0.conv, Pruning Rate: 0.23
Module: inception4b.branch3.1.conv, Pruning Rate: 0.21
Module: inception4b.branch4.1.conv, Pruning Rate: 0.21
Module: inception4c.branch1.conv, Pruning Rate: 0.18
Module: inception4c.branch2.0.conv, Pruning Rate: 0.18
Module: inception4c.branch2.1.conv, Pruning Rate: 0.21
Module: inception4c.branch3.0.conv, Pruning Rate: 0.23
Module: inception4c.branch3.1.conv, Pruning Rate: 0.23
Module: inception4c.branch4.1.conv, Pruning Rate: 0.18
Module: inception4d.branch1.conv, Pruning Rate: 0.21
Module: inception4d.branch2.0.conv, Pruning Rate: 0.18
Module: inception4d.branch2.1.conv, Pruning Rate: 0.21
Module: inception4d.branch3.0.conv, Pruning Rate: 0.23
Module: inception4d.branch3.1.conv, Pruning Rate: 0.23
Module: inception4d.branch4.1.conv, Pruning Rate: 0.21
Module: inception4e.branch1.conv, Pruning Rate: 0.18
Module: inception4e.branch2.0.conv, Pruning Rate: 0.18
Module: inception4e.branch2.1.conv, Pruning Rate: 0.21
Module: inception4e.branch3.0.conv, Pruning Rate: 0.23
Module: inception4e.branch3.1.conv, Pruning Rate: 0.21
Module: inception4e.branch4.1.conv, Pruning Rate: 0.18
Module: inception5a.branch1.conv, Pruning Rate: 0.18
Module: inception5a.branch2.0.conv, Pruning Rate: 0.21
Module: inception5a.branch2.1.conv, Pruning Rate: 0.18
Module: inception5a.branch3.0.conv, Pruning Rate: 0.23
Module: inception5a.branch3.1.conv, Pruning Rate: 0.18
Module: inception5a.branch4.1.conv, Pruning Rate: 0.18
Module: inception5b.branch1.conv, Pruning Rate: 0.21
Module: inception5b.branch2.0.conv, Pruning Rate: 0.18
Module: inception5b.branch2.1.conv, Pruning Rate: 0.18
Module: inception5b.branch3.0.conv, Pruning Rate: 0.21
Module: inception5b.branch3.1.conv, Pruning Rate: 0.23
Module: inception5b.branch4.1.conv, Pruning Rate: 0.23

--------------------------------------------------------


 Avg Pruning Rate: 0.2 

Actual Pruning Rate: 0.18868790640283828
Average Pruning Accuracy:  0.2  Accuracy:  0.69566
Epoch [1/60], Training Loss: 5.581928839584916, Training Loss w/o Aux: 1.458143935710689, Learning Rate: 0.009000000000000001, Validation Accuracy: 0.6948
Epoch [2/60], Training Loss: 5.200342897769037, Training Loss w/o Aux: 1.4256968613699463, Learning Rate: 0.008100000000000001, Validation Accuracy: 0.69466
Epoch [3/60], Training Loss: 4.805606089744526, Training Loss w/o Aux: 1.414405022167471, Learning Rate: 0.007290000000000001, Validation Accuracy: 0.69604
Epoch [4/60], Training Loss: 4.517784140090721, Training Loss w/o Aux: 1.4068511837380835, Learning Rate: 0.006561000000000002, Validation Accuracy: 0.69772
Epoch [5/60], Training Loss: 4.302016742818541, Training Loss w/o Aux: 1.4000035197909437, Learning Rate: 0.005904900000000002, Validation Accuracy: 0.69798
Epoch [6/60], Training Loss: 4.1399266573084335, Training Loss w/o Aux: 1.3966353547324166, Learning Rate: 0.005314410000000002, Validation Accuracy: 0.69912
Epoch [7/60], Training Loss: 4.014577283643916, Training Loss w/o Aux: 1.3936146490580315, Learning Rate: 0.004782969000000002, Validation Accuracy: 0.6979
Epoch [8/60], Training Loss: 3.912053468403896, Training Loss w/o Aux: 1.3895497569814883, Learning Rate: 0.004304672100000002, Validation Accuracy: 0.69822
Epoch [9/60], Training Loss: 3.826926226715475, Training Loss w/o Aux: 1.3863398704999024, Learning Rate: 0.003874204890000002, Validation Accuracy: 0.69586
Epoch [10/60], Training Loss: 3.7572633722752933, Training Loss w/o Aux: 1.3851476441564492, Learning Rate: 0.003486784401000002, Validation Accuracy: 0.69772
Epoch [11/60], Training Loss: 3.698151053909727, Training Loss w/o Aux: 1.3830903142657907, Learning Rate: 0.003138105960900002, Validation Accuracy: 0.69496
Epoch [12/60], Training Loss: 3.6437644332257264, Training Loss w/o Aux: 1.380455844200316, Learning Rate: 0.0028242953648100018, Validation Accuracy: 0.69456
Epoch [13/60], Training Loss: 3.6032113956754794, Training Loss w/o Aux: 1.3818498640189532, Learning Rate: 0.0025418658283290017, Validation Accuracy: 0.69958
Epoch [14/60], Training Loss: 3.557526926070568, Training Loss w/o Aux: 1.3781103944013806, Learning Rate: 0.0022876792454961017, Validation Accuracy: 0.69488
Epoch [15/60], Training Loss: 3.5177951092986346, Training Loss w/o Aux: 1.374625348552908, Learning Rate: 0.0020589113209464917, Validation Accuracy: 0.69746
Epoch [16/60], Training Loss: 3.4884000008893974, Training Loss w/o Aux: 1.37605421615059, Learning Rate: 0.0018530201888518425, Validation Accuracy: 0.69386
Epoch [17/60], Training Loss: 3.4567609102483727, Training Loss w/o Aux: 1.374087388061217, Learning Rate: 0.0016677181699666583, Validation Accuracy: 0.69292
Epoch [18/60], Training Loss: 3.4348712087999584, Training Loss w/o Aux: 1.3763974546516724, Learning Rate: 0.0015009463529699924, Validation Accuracy: 0.69336
Epoch [19/60], Training Loss: 3.403531616938699, Training Loss w/o Aux: 1.371401683846391, Learning Rate: 0.0013508517176729932, Validation Accuracy: 0.6922
Epoch [20/60], Training Loss: 3.3811762504899687, Training Loss w/o Aux: 1.3730833934539444, Learning Rate: 0.001215766545905694, Validation Accuracy: 0.69362
Epoch [21/60], Training Loss: 3.359827299815504, Training Loss w/o Aux: 1.370364045093962, Learning Rate: 0.0010941898913151245, Validation Accuracy: 0.69312
Epoch [22/60], Training Loss: 3.337249649286818, Training Loss w/o Aux: 1.368480850783413, Learning Rate: 0.0009847709021836122, Validation Accuracy: 0.6923
Epoch [23/60], Training Loss: 3.322529449866885, Training Loss w/o Aux: 1.3699090170841235, Learning Rate: 0.0008862938119652509, Validation Accuracy: 0.6936
Epoch [24/60], Training Loss: 3.3075314839411023, Training Loss w/o Aux: 1.371604909512389, Learning Rate: 0.0007976644307687258, Validation Accuracy: 0.6924
Epoch [25/60], Training Loss: 3.287492721394495, Training Loss w/o Aux: 1.3676777794199373, Learning Rate: 0.0007178979876918532, Validation Accuracy: 0.69368
Epoch [26/60], Training Loss: 3.2748094841321276, Training Loss w/o Aux: 1.3697976821761637, Learning Rate: 0.0006461081889226679, Validation Accuracy: 0.69162
Epoch [27/60], Training Loss: 3.259847412577208, Training Loss w/o Aux: 1.3693464811013312, Learning Rate: 0.0005814973700304011, Validation Accuracy: 0.69336
Epoch [28/60], Training Loss: 3.2447490696188144, Training Loss w/o Aux: 1.3676457553670198, Learning Rate: 0.0005233476330273611, Validation Accuracy: 0.69394
Epoch [29/60], Training Loss: 3.2335124253535894, Training Loss w/o Aux: 1.367796846977992, Learning Rate: 0.000471012869724625, Validation Accuracy: 0.68974
Epoch [30/60], Training Loss: 3.221142942534542, Training Loss w/o Aux: 1.3680988385408406, Learning Rate: 0.0004239115827521625, Validation Accuracy: 0.6916
Epoch [31/60], Training Loss: 3.2090988051749596, Training Loss w/o Aux: 1.36739596329389, Learning Rate: 0.00038152042447694626, Validation Accuracy: 0.68978
Epoch [32/60], Training Loss: 3.198761994265357, Training Loss w/o Aux: 1.3672646962866248, Learning Rate: 0.00034336838202925164, Validation Accuracy: 0.6942
Epoch [33/60], Training Loss: 3.1878820019223757, Training Loss w/o Aux: 1.3662641807349962, Learning Rate: 0.0003090315438263265, Validation Accuracy: 0.6932
Epoch [34/60], Training Loss: 3.1774897335030383, Training Loss w/o Aux: 1.366174942898767, Learning Rate: 0.00027812838944369386, Validation Accuracy: 0.68914
Epoch [35/60], Training Loss: 3.169578221642491, Training Loss w/o Aux: 1.367366167200542, Learning Rate: 0.0002503155504993245, Validation Accuracy: 0.68506
Epoch [36/60], Training Loss: 3.158671981888892, Training Loss w/o Aux: 1.3652505146384013, Learning Rate: 0.00022528399544939206, Validation Accuracy: 0.68818
Epoch [37/60], Training Loss: 3.14962311855312, Training Loss w/o Aux: 1.365333450151402, Learning Rate: 0.00020275559590445286, Validation Accuracy: 0.68936
Epoch [38/60], Training Loss: 3.1459951797128616, Training Loss w/o Aux: 1.3672528054905004, Learning Rate: 0.00018248003631400757, Validation Accuracy: 0.68748
Epoch [39/60], Training Loss: 3.1360769830915833, Training Loss w/o Aux: 1.3664061008055663, Learning Rate: 0.00016423203268260683, Validation Accuracy: 0.68922
Epoch [40/60], Training Loss: 3.1285802974533232, Training Loss w/o Aux: 1.36621096958015, Learning Rate: 0.00014780882941434616, Validation Accuracy: 0.68702
Epoch [41/60], Training Loss: 3.123261122713557, Training Loss w/o Aux: 1.3678664043419178, Learning Rate: 0.00013302794647291155, Validation Accuracy: 0.68988
Epoch [42/60], Training Loss: 3.113664608304705, Training Loss w/o Aux: 1.3664782480862916, Learning Rate: 0.00011972515182562039, Validation Accuracy: 0.68904
Epoch [43/60], Training Loss: 3.1053466324754284, Training Loss w/o Aux: 1.3656584053609335, Learning Rate: 0.00010775263664305835, Validation Accuracy: 0.69078
Epoch [44/60], Training Loss: 3.1046104958298226, Training Loss w/o Aux: 1.3678983125978208, Learning Rate: 9.697737297875251e-05, Validation Accuracy: 0.68594
Epoch [45/60], Training Loss: 3.093777456606574, Training Loss w/o Aux: 1.3658049546858804, Learning Rate: 8.727963568087727e-05, Validation Accuracy: 0.68448
Epoch [46/60], Training Loss: 3.0904983356528044, Training Loss w/o Aux: 1.3668493789170622, Learning Rate: 7.855167211278955e-05, Validation Accuracy: 0.68846
Epoch [47/60], Training Loss: 3.0841903813486273, Training Loss w/o Aux: 1.3670929427841991, Learning Rate: 7.06965049015106e-05, Validation Accuracy: 0.6872
Epoch [48/60], Training Loss: 3.0772675569865693, Training Loss w/o Aux: 1.3651808255296902, Learning Rate: 6.362685441135955e-05, Validation Accuracy: 0.68376
Epoch [49/60], Training Loss: 3.070907013532296, Training Loss w/o Aux: 1.3664441968978065, Learning Rate: 5.7264168970223595e-05, Validation Accuracy: 0.6867
Epoch [50/60], Training Loss: 3.0663702122778527, Training Loss w/o Aux: 1.3664483946929242, Learning Rate: 5.153775207320124e-05, Validation Accuracy: 0.68718
Epoch [51/60], Training Loss: 3.062154849219458, Training Loss w/o Aux: 1.3668267625304105, Learning Rate: 4.6383976865881114e-05, Validation Accuracy: 0.68494
Epoch [52/60], Training Loss: 3.0573374795108568, Training Loss w/o Aux: 1.3661078033848402, Learning Rate: 4.1745579179293e-05, Validation Accuracy: 0.68738
Epoch [53/60], Training Loss: 3.0543145561018172, Training Loss w/o Aux: 1.3677281600538818, Learning Rate: 3.75710212613637e-05, Validation Accuracy: 0.68658
Epoch [54/60], Training Loss: 3.048046380683418, Training Loss w/o Aux: 1.3669948456218066, Learning Rate: 3.381391913522733e-05, Validation Accuracy: 0.68416
Epoch [55/60], Training Loss: 3.04487527463686, Training Loss w/o Aux: 1.3673553641698306, Learning Rate: 3.0432527221704597e-05, Validation Accuracy: 0.68706
Epoch [56/60], Training Loss: 3.0410555369466414, Training Loss w/o Aux: 1.3676487897132967, Learning Rate: 2.7389274499534138e-05, Validation Accuracy: 0.68612
Epoch [57/60], Training Loss: 3.0373187445774814, Training Loss w/o Aux: 1.3683469217370734, Learning Rate: 2.4650347049580723e-05, Validation Accuracy: 0.68332
Epoch [58/60], Training Loss: 3.0345414729132165, Training Loss w/o Aux: 1.3696194371866028, Learning Rate: 2.218531234462265e-05, Validation Accuracy: 0.68292
Epoch [59/60], Training Loss: 3.0276058402162938, Training Loss w/o Aux: 1.3678133595667468, Learning Rate: 1.9966781110160387e-05, Validation Accuracy: 0.68456
Epoch [60/60], Training Loss: 3.0227548386955587, Training Loss w/o Aux: 1.366764553801641, Learning Rate: 1.797010299914435e-05, Validation Accuracy: 0.68374
Accuracy after retraining: 0.68374

------------------- Pruning Modules -------------------

Module: inception3a.branch1.conv, Pruning Rate: 0.27
Module: inception3a.branch2.0.conv, Pruning Rate: 0.32
Module: inception3a.branch2.1.conv, Pruning Rate: 0.42
Module: inception3a.branch3.0.conv, Pruning Rate: 0.42
Module: inception3a.branch3.1.conv, Pruning Rate: 0.48
Module: inception3a.branch4.1.conv, Pruning Rate: 0.32
Module: inception3b.branch1.conv, Pruning Rate: 0.37
Module: inception3b.branch2.0.conv, Pruning Rate: 0.37
Module: inception3b.branch2.1.conv, Pruning Rate: 0.42
Module: inception3b.branch3.0.conv, Pruning Rate: 0.48
Module: inception3b.branch3.1.conv, Pruning Rate: 0.42
Module: inception3b.branch4.1.conv, Pruning Rate: 0.37
Module: inception4a.branch1.conv, Pruning Rate: 0.32
Module: inception4a.branch2.0.conv, Pruning Rate: 0.42
Module: inception4a.branch2.1.conv, Pruning Rate: 0.48
Module: inception4a.branch3.0.conv, Pruning Rate: 0.48
Module: inception4a.branch3.1.conv, Pruning Rate: 0.32
Module: inception4a.branch4.1.conv, Pruning Rate: 0.32
Module: inception4b.branch1.conv, Pruning Rate: 0.37
Module: inception4b.branch2.0.conv, Pruning Rate: 0.48
Module: inception4b.branch2.1.conv, Pruning Rate: 0.42
Module: inception4b.branch3.0.conv, Pruning Rate: 0.48
Module: inception4b.branch3.1.conv, Pruning Rate: 0.42
Module: inception4b.branch4.1.conv, Pruning Rate: 0.42
Module: inception4c.branch1.conv, Pruning Rate: 0.37
Module: inception4c.branch2.0.conv, Pruning Rate: 0.37
Module: inception4c.branch2.1.conv, Pruning Rate: 0.42
Module: inception4c.branch3.0.conv, Pruning Rate: 0.48
Module: inception4c.branch3.1.conv, Pruning Rate: 0.48
Module: inception4c.branch4.1.conv, Pruning Rate: 0.37
Module: inception4d.branch1.conv, Pruning Rate: 0.42
Module: inception4d.branch2.0.conv, Pruning Rate: 0.37
Module: inception4d.branch2.1.conv, Pruning Rate: 0.42
Module: inception4d.branch3.0.conv, Pruning Rate: 0.48
Module: inception4d.branch3.1.conv, Pruning Rate: 0.48
Module: inception4d.branch4.1.conv, Pruning Rate: 0.42
Module: inception4e.branch1.conv, Pruning Rate: 0.37
Module: inception4e.branch2.0.conv, Pruning Rate: 0.37
Module: inception4e.branch2.1.conv, Pruning Rate: 0.42
Module: inception4e.branch3.0.conv, Pruning Rate: 0.48
Module: inception4e.branch3.1.conv, Pruning Rate: 0.42
Module: inception4e.branch4.1.conv, Pruning Rate: 0.37
Module: inception5a.branch1.conv, Pruning Rate: 0.37
Module: inception5a.branch2.0.conv, Pruning Rate: 0.42
Module: inception5a.branch2.1.conv, Pruning Rate: 0.37
Module: inception5a.branch3.0.conv, Pruning Rate: 0.48
Module: inception5a.branch3.1.conv, Pruning Rate: 0.37
Module: inception5a.branch4.1.conv, Pruning Rate: 0.37
Module: inception5b.branch1.conv, Pruning Rate: 0.42
Module: inception5b.branch2.0.conv, Pruning Rate: 0.37
Module: inception5b.branch2.1.conv, Pruning Rate: 0.37
Module: inception5b.branch3.0.conv, Pruning Rate: 0.42
Module: inception5b.branch3.1.conv, Pruning Rate: 0.48
Module: inception5b.branch4.1.conv, Pruning Rate: 0.48

--------------------------------------------------------


 Avg Pruning Rate: 0.4 

Actual Pruning Rate: 0.383346122315363
Average Pruning Accuracy:  0.4  Accuracy:  0.66294
Epoch [1/60], Training Loss: 5.619787942137629, Training Loss w/o Aux: 1.4952098615270286, Learning Rate: 1.6173092699229914e-05, Validation Accuracy: 0.69414
Epoch [2/60], Training Loss: 5.2348050827722306, Training Loss w/o Aux: 1.4533826602197262, Learning Rate: 1.4555783429306922e-05, Validation Accuracy: 0.69348
Epoch [3/60], Training Loss: 4.841600077196796, Training Loss w/o Aux: 1.438802827064397, Learning Rate: 1.310020508637623e-05, Validation Accuracy: 0.69834
Epoch [4/60], Training Loss: 4.552372468174105, Training Loss w/o Aux: 1.4288399655843473, Learning Rate: 1.1790184577738607e-05, Validation Accuracy: 0.69596
Epoch [5/60], Training Loss: 4.335965762641931, Training Loss w/o Aux: 1.420669932931867, Learning Rate: 1.0611166119964747e-05, Validation Accuracy: 0.6946
Epoch [6/60], Training Loss: 4.174518281375627, Training Loss w/o Aux: 1.4158277692631869, Learning Rate: 9.550049507968273e-06, Validation Accuracy: 0.69842
Epoch [7/60], Training Loss: 4.0452110186860875, Training Loss w/o Aux: 1.4102834912849027, Learning Rate: 8.595044557171446e-06, Validation Accuracy: 0.69666
Epoch [8/60], Training Loss: 3.94440973532737, Training Loss w/o Aux: 1.40700617316172, Learning Rate: 7.735540101454301e-06, Validation Accuracy: 0.69878
Epoch [9/60], Training Loss: 3.8580630449687034, Training Loss w/o Aux: 1.4032031225686499, Learning Rate: 6.9619860913088715e-06, Validation Accuracy: 0.69664
Epoch [10/60], Training Loss: 3.787238533745304, Training Loss w/o Aux: 1.3996092144670467, Learning Rate: 6.265787482177985e-06, Validation Accuracy: 0.69746
Epoch [11/60], Training Loss: 3.7287250491262425, Training Loss w/o Aux: 1.4000806530546934, Learning Rate: 5.639208733960187e-06, Validation Accuracy: 0.69386
Epoch [12/60], Training Loss: 3.6776169003133994, Training Loss w/o Aux: 1.3989089633251526, Learning Rate: 5.075287860564168e-06, Validation Accuracy: 0.69752
Epoch [13/60], Training Loss: 3.6308322109558184, Training Loss w/o Aux: 1.3965700272318484, Learning Rate: 4.5677590745077515e-06, Validation Accuracy: 0.69396
Epoch [14/60], Training Loss: 3.5899376608844475, Training Loss w/o Aux: 1.3946015481432426, Learning Rate: 4.110983167056976e-06, Validation Accuracy: 0.69612
Epoch [15/60], Training Loss: 3.550460711382095, Training Loss w/o Aux: 1.391149052707498, Learning Rate: 3.6998848503512788e-06, Validation Accuracy: 0.69872
Epoch [16/60], Training Loss: 3.5146315659732537, Training Loss w/o Aux: 1.389010256767654, Learning Rate: 3.329896365316151e-06, Validation Accuracy: 0.69368
Epoch [17/60], Training Loss: 3.490811969864939, Training Loss w/o Aux: 1.3928019566330618, Learning Rate: 2.9969067287845362e-06, Validation Accuracy: 0.69518
Epoch [18/60], Training Loss: 3.4618578040286585, Training Loss w/o Aux: 1.389926496210936, Learning Rate: 2.6972160559060827e-06, Validation Accuracy: 0.6951
Epoch [19/60], Training Loss: 3.43438646719284, Training Loss w/o Aux: 1.3875273464383249, Learning Rate: 2.4274944503154745e-06, Validation Accuracy: 0.6939
Epoch [20/60], Training Loss: 3.4096650931251338, Training Loss w/o Aux: 1.3855941411807875, Learning Rate: 2.1847450052839273e-06, Validation Accuracy: 0.69592
Epoch [21/60], Training Loss: 3.3924556143908653, Training Loss w/o Aux: 1.3880221554182284, Learning Rate: 1.9662705047555346e-06, Validation Accuracy: 0.69354
Epoch [22/60], Training Loss: 3.3729965447164676, Training Loss w/o Aux: 1.387700110444633, Learning Rate: 1.7696434542799813e-06, Validation Accuracy: 0.69536
Epoch [23/60], Training Loss: 3.3498755967966005, Training Loss w/o Aux: 1.383232987853647, Learning Rate: 1.5926791088519833e-06, Validation Accuracy: 0.68942
Epoch [24/60], Training Loss: 3.337087213868015, Training Loss w/o Aux: 1.3855914322690823, Learning Rate: 1.433411197966785e-06, Validation Accuracy: 0.69068
Epoch [25/60], Training Loss: 3.3216926115163496, Training Loss w/o Aux: 1.384866084683512, Learning Rate: 1.2900700781701065e-06, Validation Accuracy: 0.69172
Epoch [26/60], Training Loss: 3.3028149558347546, Training Loss w/o Aux: 1.3837065278489293, Learning Rate: 1.161063070353096e-06, Validation Accuracy: 0.69128
Epoch [27/60], Training Loss: 3.2903600353879736, Training Loss w/o Aux: 1.3831260640467067, Learning Rate: 1.0449567633177863e-06, Validation Accuracy: 0.6931
Epoch [28/60], Training Loss: 3.2742116704236284, Training Loss w/o Aux: 1.381721681559499, Learning Rate: 9.404610869860078e-07, Validation Accuracy: 0.69258
Epoch [29/60], Training Loss: 3.2623024165231347, Training Loss w/o Aux: 1.3822283056818745, Learning Rate: 8.46414978287407e-07, Validation Accuracy: 0.68992
Epoch [30/60], Training Loss: 3.254285849823629, Training Loss w/o Aux: 1.3842895008584148, Learning Rate: 7.617734804586663e-07, Validation Accuracy: 0.69076
Epoch [31/60], Training Loss: 3.242394512427867, Training Loss w/o Aux: 1.3836635480287705, Learning Rate: 6.855961324127997e-07, Validation Accuracy: 0.69194
Epoch [32/60], Training Loss: 3.2281811889300087, Training Loss w/o Aux: 1.3806719939684176, Learning Rate: 6.170365191715197e-07, Validation Accuracy: 0.693
Epoch [33/60], Training Loss: 3.2184741951295197, Training Loss w/o Aux: 1.3810274917615593, Learning Rate: 5.553328672543678e-07, Validation Accuracy: 0.69294
Epoch [34/60], Training Loss: 3.2090314417193135, Training Loss w/o Aux: 1.3811250223254976, Learning Rate: 4.99799580528931e-07, Validation Accuracy: 0.68996
Epoch [35/60], Training Loss: 3.1991046331111983, Training Loss w/o Aux: 1.3810903962609626, Learning Rate: 4.498196224760379e-07, Validation Accuracy: 0.69322
Epoch [36/60], Training Loss: 3.1940392450276804, Training Loss w/o Aux: 1.3837786041626696, Learning Rate: 4.0483766022843414e-07, Validation Accuracy: 0.69216
Epoch [37/60], Training Loss: 3.18417119391494, Training Loss w/o Aux: 1.3827945220147662, Learning Rate: 3.643538942055907e-07, Validation Accuracy: 0.69262
Epoch [38/60], Training Loss: 3.175777709118457, Training Loss w/o Aux: 1.3816754885553086, Learning Rate: 3.2791850478503163e-07, Validation Accuracy: 0.69118
Epoch [39/60], Training Loss: 3.170120683961225, Training Loss w/o Aux: 1.3830853526034141, Learning Rate: 2.951266543065285e-07, Validation Accuracy: 0.69018
Epoch [40/60], Training Loss: 3.162072444512349, Training Loss w/o Aux: 1.3831450664767233, Learning Rate: 2.6561398887587566e-07, Validation Accuracy: 0.69076
Epoch [41/60], Training Loss: 3.1520696555071033, Training Loss w/o Aux: 1.3805996963570246, Learning Rate: 2.390525899882881e-07, Validation Accuracy: 0.68856
Epoch [42/60], Training Loss: 3.1483968125796005, Training Loss w/o Aux: 1.3828573267482975, Learning Rate: 2.151473309894593e-07, Validation Accuracy: 0.68696
Epoch [43/60], Training Loss: 3.140241267941003, Training Loss w/o Aux: 1.3822744336622748, Learning Rate: 1.936325978905134e-07, Validation Accuracy: 0.68806
Epoch [44/60], Training Loss: 3.13514470025035, Training Loss w/o Aux: 1.3831903561867847, Learning Rate: 1.7426933810146205e-07, Validation Accuracy: 0.69212
Epoch [45/60], Training Loss: 3.126713352230048, Training Loss w/o Aux: 1.3809550088273024, Learning Rate: 1.5684240429131584e-07, Validation Accuracy: 0.68926
Epoch [46/60], Training Loss: 3.1223425990945155, Training Loss w/o Aux: 1.3825219437570788, Learning Rate: 1.4115816386218426e-07, Validation Accuracy: 0.6881
Epoch [47/60], Training Loss: 3.1167943983814532, Training Loss w/o Aux: 1.3821857906993376, Learning Rate: 1.2704234747596583e-07, Validation Accuracy: 0.68792
Epoch [48/60], Training Loss: 3.11060897679176, Training Loss w/o Aux: 1.381991210392584, Learning Rate: 1.1433811272836925e-07, Validation Accuracy: 0.68892
Epoch [49/60], Training Loss: 3.1088847811970752, Training Loss w/o Aux: 1.3848091507305786, Learning Rate: 1.0290430145553233e-07, Validation Accuracy: 0.68794
Epoch [50/60], Training Loss: 3.1075609989939945, Training Loss w/o Aux: 1.3864931240759717, Learning Rate: 9.26138713099791e-08, Validation Accuracy: 0.69014
Epoch [51/60], Training Loss: 3.0956726557058656, Training Loss w/o Aux: 1.382883027718491, Learning Rate: 8.335248417898118e-08, Validation Accuracy: 0.69036
Epoch [52/60], Training Loss: 3.090983800566962, Training Loss w/o Aux: 1.382453536038061, Learning Rate: 7.501723576108307e-08, Validation Accuracy: 0.68834
Epoch [53/60], Training Loss: 3.085633549040902, Training Loss w/o Aux: 1.3814743456991536, Learning Rate: 6.751551218497476e-08, Validation Accuracy: 0.68716
Epoch [54/60], Training Loss: 3.084189749440538, Training Loss w/o Aux: 1.3846166565327964, Learning Rate: 6.076396096647729e-08, Validation Accuracy: 0.69206
Epoch [55/60], Training Loss: 3.079954448929199, Training Loss w/o Aux: 1.383861132435823, Learning Rate: 5.468756486982956e-08, Validation Accuracy: 0.68938
Epoch [56/60], Training Loss: 3.077931703036214, Training Loss w/o Aux: 1.3862075343250169, Learning Rate: 4.921880838284661e-08, Validation Accuracy: 0.68146
Epoch [57/60], Training Loss: 3.0714940843745087, Training Loss w/o Aux: 1.384461617955724, Learning Rate: 4.4296927544561945e-08, Validation Accuracy: 0.68306
Epoch [58/60], Training Loss: 3.066620812405119, Training Loss w/o Aux: 1.3832676134119979, Learning Rate: 3.986723479010575e-08, Validation Accuracy: 0.68418
Epoch [59/60], Training Loss: 3.0631419054956175, Training Loss w/o Aux: 1.3846269612284208, Learning Rate: 3.588051131109518e-08, Validation Accuracy: 0.68476
Epoch [60/60], Training Loss: 3.061272023095512, Training Loss w/o Aux: 1.385707792416118, Learning Rate: 3.2292460179985664e-08, Validation Accuracy: 0.68714
Accuracy after retraining: 0.68714

------------------- Pruning Modules -------------------

Module: inception3a.branch1.conv, Pruning Rate: 0.4
Module: inception3a.branch2.0.conv, Pruning Rate: 0.47
Module: inception3a.branch2.1.conv, Pruning Rate: 0.63
Module: inception3a.branch3.0.conv, Pruning Rate: 0.63
Module: inception3a.branch3.1.conv, Pruning Rate: 0.71
Module: inception3a.branch4.1.conv, Pruning Rate: 0.47
Module: inception3b.branch1.conv, Pruning Rate: 0.55
Module: inception3b.branch2.0.conv, Pruning Rate: 0.55
Module: inception3b.branch2.1.conv, Pruning Rate: 0.63
Module: inception3b.branch3.0.conv, Pruning Rate: 0.71
Module: inception3b.branch3.1.conv, Pruning Rate: 0.63
Module: inception3b.branch4.1.conv, Pruning Rate: 0.55
Module: inception4a.branch1.conv, Pruning Rate: 0.47
Module: inception4a.branch2.0.conv, Pruning Rate: 0.63
Module: inception4a.branch2.1.conv, Pruning Rate: 0.71
Module: inception4a.branch3.0.conv, Pruning Rate: 0.71
Module: inception4a.branch3.1.conv, Pruning Rate: 0.47
Module: inception4a.branch4.1.conv, Pruning Rate: 0.47
Module: inception4b.branch1.conv, Pruning Rate: 0.55
Module: inception4b.branch2.0.conv, Pruning Rate: 0.71
Module: inception4b.branch2.1.conv, Pruning Rate: 0.63
Module: inception4b.branch3.0.conv, Pruning Rate: 0.71
Module: inception4b.branch3.1.conv, Pruning Rate: 0.63
Module: inception4b.branch4.1.conv, Pruning Rate: 0.63
Module: inception4c.branch1.conv, Pruning Rate: 0.55
Module: inception4c.branch2.0.conv, Pruning Rate: 0.55
Module: inception4c.branch2.1.conv, Pruning Rate: 0.63
Module: inception4c.branch3.0.conv, Pruning Rate: 0.71
Module: inception4c.branch3.1.conv, Pruning Rate: 0.71
Module: inception4c.branch4.1.conv, Pruning Rate: 0.55
Module: inception4d.branch1.conv, Pruning Rate: 0.63
Module: inception4d.branch2.0.conv, Pruning Rate: 0.55
Module: inception4d.branch2.1.conv, Pruning Rate: 0.63
Module: inception4d.branch3.0.conv, Pruning Rate: 0.71
Module: inception4d.branch3.1.conv, Pruning Rate: 0.71
Module: inception4d.branch4.1.conv, Pruning Rate: 0.63
Module: inception4e.branch1.conv, Pruning Rate: 0.55
Module: inception4e.branch2.0.conv, Pruning Rate: 0.55
Module: inception4e.branch2.1.conv, Pruning Rate: 0.63
Module: inception4e.branch3.0.conv, Pruning Rate: 0.71
Module: inception4e.branch3.1.conv, Pruning Rate: 0.63
Module: inception4e.branch4.1.conv, Pruning Rate: 0.55
Module: inception5a.branch1.conv, Pruning Rate: 0.55
Module: inception5a.branch2.0.conv, Pruning Rate: 0.63
Module: inception5a.branch2.1.conv, Pruning Rate: 0.55
Module: inception5a.branch3.0.conv, Pruning Rate: 0.71
Module: inception5a.branch3.1.conv, Pruning Rate: 0.55
Module: inception5a.branch4.1.conv, Pruning Rate: 0.55
Module: inception5b.branch1.conv, Pruning Rate: 0.63
Module: inception5b.branch2.0.conv, Pruning Rate: 0.55
Module: inception5b.branch2.1.conv, Pruning Rate: 0.55
Module: inception5b.branch3.0.conv, Pruning Rate: 0.63
Module: inception5b.branch3.1.conv, Pruning Rate: 0.71
Module: inception5b.branch4.1.conv, Pruning Rate: 0.71

--------------------------------------------------------


 Avg Pruning Rate: 0.6 

Actual Pruning Rate: 0.5717649005047509
Average Pruning Accuracy:  0.6  Accuracy:  0.36514
Epoch [1/60], Training Loss: 5.777779967387795, Training Loss w/o Aux: 1.650217544688296, Learning Rate: 2.90632141619871e-08, Validation Accuracy: 0.6743
Epoch [2/60], Training Loss: 5.356660045768988, Training Loss w/o Aux: 1.5587230819554843, Learning Rate: 2.615689274578839e-08, Validation Accuracy: 0.6847
Epoch [3/60], Training Loss: 4.955396120978683, Training Loss w/o Aux: 1.5297894395842444, Learning Rate: 2.354120347120955e-08, Validation Accuracy: 0.68462
Epoch [4/60], Training Loss: 4.660828923712197, Training Loss w/o Aux: 1.5112420191880598, Learning Rate: 2.1187083124088596e-08, Validation Accuracy: 0.68526
Epoch [5/60], Training Loss: 4.442711035990193, Training Loss w/o Aux: 1.4984517084074158, Learning Rate: 1.9068374811679737e-08, Validation Accuracy: 0.68684
Epoch [6/60], Training Loss: 4.275810222857267, Training Loss w/o Aux: 1.4887821548515559, Learning Rate: 1.7161537330511763e-08, Validation Accuracy: 0.68684
Epoch [7/60], Training Loss: 4.151574825547366, Training Loss w/o Aux: 1.484829719029222, Learning Rate: 1.5445383597460585e-08, Validation Accuracy: 0.68654
Epoch [8/60], Training Loss: 4.047043115516942, Training Loss w/o Aux: 1.4792101451335629, Learning Rate: 1.3900845237714528e-08, Validation Accuracy: 0.68966
Epoch [9/60], Training Loss: 3.9605983452954625, Training Loss w/o Aux: 1.4736045815570928, Learning Rate: 1.2510760713943076e-08, Validation Accuracy: 0.6889
Epoch [10/60], Training Loss: 3.8883764443667648, Training Loss w/o Aux: 1.469316022172652, Learning Rate: 1.1259684642548768e-08, Validation Accuracy: 0.68888
Epoch [11/60], Training Loss: 3.826494522249083, Training Loss w/o Aux: 1.4658428417217817, Learning Rate: 1.0133716178293891e-08, Validation Accuracy: 0.68994
Epoch [12/60], Training Loss: 3.774312715428444, Training Loss w/o Aux: 1.463483600084545, Learning Rate: 9.120344560464503e-09, Validation Accuracy: 0.69082
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 468857.0 ON galvani-cn110 CANCELLED AT 2024-07-09T11:55:20 ***
slurmstepd: error: *** JOB 468857 ON galvani-cn110 CANCELLED AT 2024-07-09T11:55:20 ***
