JobId=467031 JobName=name
   UserId=wzz745(4834) GroupId=wichmann(4014) MCS_label=N/A
   Priority=74268 Nice=0 Account=wichmann QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:00 TimeLimit=3-00:00:00 TimeMin=N/A
   SubmitTime=2024-07-05T11:51:38 EligibleTime=2024-07-05T11:51:38
   AccrueTime=2024-07-05T11:51:39
   StartTime=2024-07-05T11:51:39 EndTime=2024-07-08T11:51:39 Deadline=N/A
   PreemptEligibleTime=2024-07-05T11:52:39 PreemptTime=None
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-07-05T11:51:39 Scheduler=Main
   Partition=2080-galvani AllocNode:Sid=galvani-slurmctl:945118
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=galvani-cn119
   BatchHost=galvani-cn119
   NumNodes=1 NumCPUs=8 NumTasks=1 CPUs/Task=8 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=8,mem=40G,node=1,billing=2,gres/gpu=1
   AllocTRES=cpu=8,mem=40G,node=1,billing=2,gres/gpu=1,gres/gpu:rtx2080ti=1
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=8 MinMemoryNode=40G MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/ffcv.sh
   WorkDir=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability
   StdErr=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/slurm-467031.out
   StdIn=/dev/null
   StdOut=/mnt/qb/home/wichmann/wzz745/Network-Pruning-and-Interpretability/slurm-467031.out
   Power=
   TresPerNode=gres:gpu:1
   MailUser=jonathan.sakouhi@gmail.com MailType=BEGIN,END,FAIL
   

/usr/local/lib/python3.10/dist-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.
  warnings.warn(problem)
Train loader created in 15.21522331237793 seconds
Using cache found in /home/wichmann/wzz745/.cache/torch/hub/pytorch_vision_v0.10.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/googlenet.py:341: UserWarning: auxiliary heads in the pretrained googlenet model are NOT pretrained, so make sure to train them
  warnings.warn(
wandb: Currently logged in as: jonathan-vonrad (jonathan-von-rad). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/wichmann/wzz745/.netrc
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/wichmann/wzz745/Network-Pruning-and-Interpretability/wandb/run-20240705_115213-w4q7cv8w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-salad-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jonathan-von-rad/epic
wandb: üöÄ View run at https://wandb.ai/jonathan-von-rad/epic/runs/w4q7cv8w
Train loader created in 0.2632467746734619 seconds
Training for 10 epochs with learning rate 0.01 and optimizer Adam and scheduler ExponentialLR

########## Specific Local Structured L1 Pruning ##########

Accuracy before: 0.69938
Accuracy before:  0.69938

------------------- Pruning Modules with 0.8 -------------------


--------------------------------------------------------

Actual Pruning Rate: 0.7636763998164542
Accuracy after pruning every module with 0.8:  0.001
Epoch [1/50], Training Loss: 8.170404650112385, Training Loss w/o Aux: 4.824553634251185, Learning Rate: 0.009000000000000001, Validation Accuracy: 0.0713
Epoch [2/50], Training Loss: 6.871123795219206, Training Loss w/o Aux: 4.055426118733321, Learning Rate: 0.008100000000000001, Validation Accuracy: 0.14024
Epoch [3/50], Training Loss: 6.434522142990544, Training Loss w/o Aux: 3.7982480975461774, Learning Rate: 0.007290000000000001, Validation Accuracy: 0.16434
Epoch [4/50], Training Loss: 6.189676696835656, Training Loss w/o Aux: 3.6545927267782528, Learning Rate: 0.006561000000000002, Validation Accuracy: 0.19216
Epoch [5/50], Training Loss: 6.018864584059634, Training Loss w/o Aux: 3.5549016585060857, Learning Rate: 0.005904900000000002, Validation Accuracy: 0.20624
Epoch [6/50], Training Loss: 5.89238587701603, Training Loss w/o Aux: 3.4818588072799375, Learning Rate: 0.005314410000000002, Validation Accuracy: 0.2252
Epoch [7/50], Training Loss: 5.789844988585591, Training Loss w/o Aux: 3.422953009355293, Learning Rate: 0.004782969000000002, Validation Accuracy: 0.2515
Epoch [8/50], Training Loss: 5.706304965408452, Training Loss w/o Aux: 3.3741173190853315, Learning Rate: 0.004304672100000002, Validation Accuracy: 0.26476
Epoch [9/50], Training Loss: 5.643383020310864, Training Loss w/o Aux: 3.338912629809624, Learning Rate: 0.003874204890000002, Validation Accuracy: 0.27442
Epoch [10/50], Training Loss: 5.5921973886755705, Training Loss w/o Aux: 3.3107701158271063, Learning Rate: 0.003486784401000002, Validation Accuracy: 0.28366
Epoch [11/50], Training Loss: 5.540431522062959, Training Loss w/o Aux: 3.281270377011146, Learning Rate: 0.003138105960900002, Validation Accuracy: 0.29776
Epoch [12/50], Training Loss: 5.496921750593762, Training Loss w/o Aux: 3.256142346601003, Learning Rate: 0.0028242953648100018, Validation Accuracy: 0.3065
Epoch [13/50], Training Loss: 5.45984206607928, Training Loss w/o Aux: 3.2349248551288583, Learning Rate: 0.0025418658283290017, Validation Accuracy: 0.31258
Epoch [14/50], Training Loss: 5.431113621228367, Training Loss w/o Aux: 3.2190647901313407, Learning Rate: 0.0022876792454961017, Validation Accuracy: 0.33
Epoch [15/50], Training Loss: 5.39913024167722, Training Loss w/o Aux: 3.201194543535505, Learning Rate: 0.0020589113209464917, Validation Accuracy: 0.34144
Epoch [16/50], Training Loss: 5.3761951409563915, Training Loss w/o Aux: 3.1878253908611844, Learning Rate: 0.0018530201888518425, Validation Accuracy: 0.35144
Epoch [17/50], Training Loss: 5.351634483688039, Training Loss w/o Aux: 3.1738663340486695, Learning Rate: 0.0016677181699666583, Validation Accuracy: 0.34996
Epoch [18/50], Training Loss: 5.334526509835606, Training Loss w/o Aux: 3.164545239500857, Learning Rate: 0.0015009463529699924, Validation Accuracy: 0.36812
Epoch [19/50], Training Loss: 5.314097337211115, Training Loss w/o Aux: 3.153350073255756, Learning Rate: 0.0013508517176729932, Validation Accuracy: 0.3643
Epoch [20/50], Training Loss: 5.295067630908458, Training Loss w/o Aux: 3.1415344368674227, Learning Rate: 0.001215766545905694, Validation Accuracy: 0.37926
Epoch [21/50], Training Loss: 5.2811077021018455, Training Loss w/o Aux: 3.1337443448313587, Learning Rate: 0.0010941898913151245, Validation Accuracy: 0.37578
Epoch [22/50], Training Loss: 5.267789758851281, Training Loss w/o Aux: 3.126829681440314, Learning Rate: 0.0009847709021836122, Validation Accuracy: 0.38588
Epoch [23/50], Training Loss: 5.255534565952563, Training Loss w/o Aux: 3.1202725272445915, Learning Rate: 0.0008862938119652509, Validation Accuracy: 0.38716
Epoch [24/50], Training Loss: 5.246011676935064, Training Loss w/o Aux: 3.114353785041758, Learning Rate: 0.0007976644307687258, Validation Accuracy: 0.39738
Epoch [25/50], Training Loss: 5.232743919116061, Training Loss w/o Aux: 3.1061339406465027, Learning Rate: 0.0007178979876918532, Validation Accuracy: 0.39208
Epoch [26/50], Training Loss: 5.221475853168687, Training Loss w/o Aux: 3.099546352696712, Learning Rate: 0.0006461081889226679, Validation Accuracy: 0.39764
Epoch [27/50], Training Loss: 5.217101870937083, Training Loss w/o Aux: 3.097622635719695, Learning Rate: 0.0005814973700304011, Validation Accuracy: 0.40304
Epoch [28/50], Training Loss: 5.210108185043129, Training Loss w/o Aux: 3.093711782233057, Learning Rate: 0.0005233476330273611, Validation Accuracy: 0.41088
Epoch [29/50], Training Loss: 5.204188193602214, Training Loss w/o Aux: 3.091048518431533, Learning Rate: 0.000471012869724625, Validation Accuracy: 0.41578
Epoch [30/50], Training Loss: 5.19538092315465, Training Loss w/o Aux: 3.085347607081224, Learning Rate: 0.0004239115827521625, Validation Accuracy: 0.4214
Epoch [31/50], Training Loss: 5.186750068449214, Training Loss w/o Aux: 3.0793917917064264, Learning Rate: 0.00038152042447694626, Validation Accuracy: 0.41952
Epoch [32/50], Training Loss: 5.18467941287991, Training Loss w/o Aux: 3.0787909263639803, Learning Rate: 0.00034336838202925164, Validation Accuracy: 0.42284
Epoch [33/50], Training Loss: 5.176687829417537, Training Loss w/o Aux: 3.0744395710059678, Learning Rate: 0.0003090315438263265, Validation Accuracy: 0.42562
Epoch [34/50], Training Loss: 5.17480727658904, Training Loss w/o Aux: 3.0729314281885913, Learning Rate: 0.00027812838944369386, Validation Accuracy: 0.4297
Epoch [35/50], Training Loss: 5.172014777269, Training Loss w/o Aux: 3.070677195863536, Learning Rate: 0.0002503155504993245, Validation Accuracy: 0.42618
Epoch [36/50], Training Loss: 5.164685221696927, Training Loss w/o Aux: 3.06700902788393, Learning Rate: 0.00022528399544939206, Validation Accuracy: 0.43204
Epoch [37/50], Training Loss: 5.1650407748070375, Training Loss w/o Aux: 3.067870611076839, Learning Rate: 0.00020275559590445286, Validation Accuracy: 0.4331
Epoch [38/50], Training Loss: 5.165507215162009, Training Loss w/o Aux: 3.068443334118114, Learning Rate: 0.00018248003631400757, Validation Accuracy: 0.43628
Epoch [39/50], Training Loss: 5.156496644294015, Training Loss w/o Aux: 3.0621349822273145, Learning Rate: 0.00016423203268260683, Validation Accuracy: 0.43762
Epoch [40/50], Training Loss: 5.1567850773693475, Training Loss w/o Aux: 3.062929358035966, Learning Rate: 0.00014780882941434616, Validation Accuracy: 0.4374
Epoch [41/50], Training Loss: 5.1533386365959295, Training Loss w/o Aux: 3.061108356661248, Learning Rate: 0.00013302794647291155, Validation Accuracy: 0.43196
Epoch [42/50], Training Loss: 5.151073602067923, Training Loss w/o Aux: 3.0594298707842174, Learning Rate: 0.00011972515182562039, Validation Accuracy: 0.43512
Epoch [43/50], Training Loss: 5.14803839787485, Training Loss w/o Aux: 3.057917086891581, Learning Rate: 0.00010775263664305835, Validation Accuracy: 0.43794
Epoch [44/50], Training Loss: 5.14679710004389, Training Loss w/o Aux: 3.057012102084389, Learning Rate: 9.697737297875251e-05, Validation Accuracy: 0.44298
Epoch [45/50], Training Loss: 5.146180163659896, Training Loss w/o Aux: 3.0566377204809645, Learning Rate: 8.727963568087727e-05, Validation Accuracy: 0.43784
Epoch [46/50], Training Loss: 5.140324968190707, Training Loss w/o Aux: 3.0526345647742716, Learning Rate: 7.855167211278955e-05, Validation Accuracy: 0.44478
Epoch [47/50], Training Loss: 5.145668383041978, Training Loss w/o Aux: 3.0563447792721337, Learning Rate: 7.06965049015106e-05, Validation Accuracy: 0.44418
Epoch [48/50], Training Loss: 5.141435342207097, Training Loss w/o Aux: 3.0539805340545376, Learning Rate: 6.362685441135955e-05, Validation Accuracy: 0.44416
Epoch [49/50], Training Loss: 5.137576320881443, Training Loss w/o Aux: 3.0510567513983355, Learning Rate: 5.7264168970223595e-05, Validation Accuracy: 0.44114
Epoch [50/50], Training Loss: 5.13933163516254, Training Loss w/o Aux: 3.0527692556452686, Learning Rate: 5.153775207320124e-05, Validation Accuracy: 0.4434
Accuracy after retraining: 0.4434
removing pruning masks ...
Final pruned and retrained model saved as pruned_0.8_local_structured_SGD_retrained_50_epochs_model.pth

Resetting the model to the initial state ...
Finished pruning, retraining, and evaluation.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      accuracy ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:         epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: learning rate ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: training loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      accuracy 0.4434
wandb:         epoch 50
wandb: learning rate 5e-05
wandb: training loss 5.13933
wandb: 
wandb: üöÄ View run frosty-salad-15 at: https://wandb.ai/jonathan-von-rad/epic/runs/w4q7cv8w
wandb: Ô∏è‚ö° View job at https://wandb.ai/jonathan-von-rad/epic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjIzOTI0NjUwOQ==/version_details/v2
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240705_115213-w4q7cv8w/logs
